{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom functools import partial\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\nfrom zipfile import ZipFile\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:27:41.139175Z","iopub.execute_input":"2024-07-11T07:27:41.140235Z","iopub.status.idle":"2024-07-11T07:27:41.145601Z","shell.execute_reply.started":"2024-07-11T07:27:41.140196Z","shell.execute_reply":"2024-07-11T07:27:41.144530Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":" !pip install -q gdown","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:18:04.593701Z","iopub.execute_input":"2024-07-11T07:18:04.594460Z","iopub.status.idle":"2024-07-11T07:18:16.853886Z","shell.execute_reply.started":"2024-07-11T07:18:04.594418Z","shell.execute_reply":"2024-07-11T07:18:16.852649Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n\nclass InstanceNormalization(Layer):\n    def __init__(self, epsilon=1e-5, **kwargs):\n        super(InstanceNormalization, self).__init__(**kwargs)\n        self.epsilon = epsilon\n\n    def build(self, input_shape):\n        self.gamma = self.add_weight(name='gamma', \n                                     shape=(input_shape[-1],),\n                                     initializer='ones',\n                                     trainable=True)\n        self.beta = self.add_weight(name='beta', \n                                    shape=(input_shape[-1],),\n                                    initializer='zeros',\n                                    trainable=True)\n        super(InstanceNormalization, self).build(input_shape)\n\n    def call(self, inputs):\n        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n        normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n        return self.gamma * normalized + self.beta\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:28:00.338741Z","iopub.execute_input":"2024-07-11T07:28:00.339340Z","iopub.status.idle":"2024-07-11T07:28:00.352782Z","shell.execute_reply.started":"2024-07-11T07:28:00.339308Z","shell.execute_reply":"2024-07-11T07:28:00.351475Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\ndef log2(x):\n    return int(np.log2(x))\n\n\n# we use different batch size for different resolution, so larger image size\n# could fit into GPU memory. The keys is image resolution in log2\nbatch_sizes = {2: 16, 3: 16, 4: 16, 5: 16, 6: 16, 7: 8, 8: 4, 9: 2, 10: 1}\n# We adjust the train step accordingly\ntrain_step_ratio = {k: batch_sizes[2] / v for k, v in batch_sizes.items()}\n\n\nos.makedirs(\"celeba_gan\")\n\nurl = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\noutput = \"celeba_gan/data.zip\"\ngdown.download(url, output, quiet=True)\n\nwith ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n    zipobj.extractall(\"celeba_gan\")\n\n# Create a dataset from our folder, and rescale the images to the [0-1] range:\n\nds_train = keras.utils.image_dataset_from_directory(\n    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n)\n\n\n\ndef resize_image(res, image):\n    # only downsampling, so use nearest neighbor that is faster to run\n    image = tf.image.resize(\n        image, (res, res), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    )\n    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n    return image\n\n\ndef create_dataloader(res):\n    batch_size = batch_sizes[log2(res)]\n    dl = ds_train.map(partial(resize_image, res), num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n    dl = dl.shuffle(200).batch(batch_size, drop_remainder=True).prefetch(1).repeat()\n    return dl\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:19:23.800133Z","iopub.execute_input":"2024-07-11T07:19:23.800553Z","iopub.status.idle":"2024-07-11T07:20:30.417385Z","shell.execute_reply.started":"2024-07-11T07:19:23.800518Z","shell.execute_reply":"2024-07-11T07:20:30.416371Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Found 202599 files.\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef plot_images(images, log2_res, fname=\"\"):\n    scales = {2: 0.5, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8}\n    scale = scales[log2_res]\n\n    grid_col = min(images.shape[0], int(32 // scale))\n    grid_row = 1\n\n    f, axarr = plt.subplots(\n        grid_row, grid_col, figsize=(grid_col * scale, grid_row * scale)\n    )\n\n    for row in range(grid_row):\n        ax = axarr if grid_row == 1 else axarr[row]\n        for col in range(grid_col):\n            ax[col].imshow(images[row * grid_col + col])\n            ax[col].axis(\"off\")\n    plt.show()\n    if fname:\n        f.savefig(fname)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:21:08.435362Z","iopub.execute_input":"2024-07-11T07:21:08.437724Z","iopub.status.idle":"2024-07-11T07:21:08.449779Z","shell.execute_reply.started":"2024-07-11T07:21:08.437652Z","shell.execute_reply":"2024-07-11T07:21:08.448578Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:21:46.611361Z","iopub.execute_input":"2024-07-11T07:21:46.611778Z","iopub.status.idle":"2024-07-11T07:21:46.620670Z","shell.execute_reply.started":"2024-07-11T07:21:46.611736Z","shell.execute_reply":"2024-07-11T07:21:46.619432Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\ndef fade_in(alpha, a, b):\n    return alpha * a + (1.0 - alpha) * b\n\n\ndef wasserstein_loss(y_true, y_pred):\n    return -tf.reduce_mean(y_true * y_pred)\n\n\ndef pixel_norm(x, epsilon=1e-8):\n    return x / tf.math.sqrt(tf.reduce_mean(x ** 2, axis=-1, keepdims=True) + epsilon)\n\n\ndef minibatch_std(input_tensor, epsilon=1e-8):\n    n, h, w, c = tf.shape(input_tensor)\n    group_size = tf.minimum(4, n)\n    x = tf.reshape(input_tensor, [group_size, -1, h, w, c])\n    group_mean, group_var = tf.nn.moments(x, axes=(0), keepdims=False)\n    group_std = tf.sqrt(group_var + epsilon)\n    avg_std = tf.reduce_mean(group_std, axis=[1, 2, 3], keepdims=True)\n    x = tf.tile(avg_std, [group_size, h, w, 1])\n    return tf.concat([input_tensor, x], axis=-1)\n\n\nclass EqualizedConv(layers.Layer):\n    def __init__(self, out_channels, kernel=3, gain=2, **kwargs):\n        super().__init__(**kwargs)\n        self.kernel = kernel\n        self.out_channels = out_channels\n        self.gain = gain\n        self.pad = kernel != 1\n\n    def build(self, input_shape):\n        self.in_channels = input_shape[-1]\n        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n        self.w = self.add_weight(\n            shape=[self.kernel, self.kernel, self.in_channels, self.out_channels],\n            initializer=initializer,\n            trainable=True,\n            name=\"kernel\",\n        )\n        self.b = self.add_weight(\n            shape=(self.out_channels,), initializer=\"zeros\", trainable=True, name=\"bias\"\n        )\n        fan_in = self.kernel * self.kernel * self.in_channels\n        self.scale = tf.sqrt(self.gain / fan_in)\n\n    def call(self, inputs):\n        if self.pad:\n            x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"REFLECT\")\n        else:\n            x = inputs\n        output = (\n            tf.nn.conv2d(x, self.scale * self.w, strides=1, padding=\"VALID\") + self.b\n        )\n        return output\n\n\nclass EqualizedDense(layers.Layer):\n    def __init__(self, units, gain=2, learning_rate_multiplier=1, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.gain = gain\n        self.learning_rate_multiplier = learning_rate_multiplier\n\n    def build(self, input_shape):\n        self.in_channels = input_shape[-1]\n        initializer = keras.initializers.RandomNormal(\n            mean=0.0, stddev=1.0 / self.learning_rate_multiplier\n        )\n        self.w = self.add_weight(\n            shape=[self.in_channels, self.units],\n            initializer=initializer,\n            trainable=True,\n            name=\"kernel\",\n        )\n        self.b = self.add_weight(\n            shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"bias\"\n        )\n        fan_in = self.in_channels\n        self.scale = tf.sqrt(self.gain / fan_in)\n\n    def call(self, inputs):\n        output = tf.add(tf.matmul(inputs, self.scale * self.w), self.b)\n        return output * self.learning_rate_multiplier\n\n\nclass AddNoise(layers.Layer):\n    def build(self, input_shape):\n        n, h, w, c = input_shape[0]\n        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n        self.b = self.add_weight(\n            shape=[1, 1, 1, c], initializer=initializer, trainable=True, name=\"kernel\"\n        )\n\n    def call(self, inputs):\n        x, noise = inputs\n        output = x + self.b * noise\n        return output\n\n\nclass AdaIN(layers.Layer):\n    def __init__(self, gain=1, **kwargs):\n        super().__init__(**kwargs)\n        self.gain = gain\n\n    def build(self, input_shapes):\n        x_shape = input_shapes[0]\n        w_shape = input_shapes[1]\n\n        self.w_channels = w_shape[-1]\n        self.x_channels = x_shape[-1]\n\n        self.dense_1 = EqualizedDense(self.x_channels, gain=1)\n        self.dense_2 = EqualizedDense(self.x_channels, gain=1)\n\n    def call(self, inputs):\n        x, w = inputs\n        ys = tf.reshape(self.dense_1(w), (-1, 1, 1, self.x_channels))\n        yb = tf.reshape(self.dense_2(w), (-1, 1, 1, self.x_channels))\n        return ys * x + yb\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:29:48.119141Z","iopub.execute_input":"2024-07-11T07:29:48.119592Z","iopub.status.idle":"2024-07-11T07:29:48.143781Z","shell.execute_reply.started":"2024-07-11T07:29:48.119561Z","shell.execute_reply":"2024-07-11T07:29:48.142518Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\ndef Mapping(num_stages, input_shape=512):\n    z = layers.Input(shape=(input_shape))\n    w = pixel_norm(z)\n    for i in range(8):\n        w = EqualizedDense(512, learning_rate_multiplier=0.01)(w)\n        w = layers.LeakyReLU(0.2)(w)\n    w = tf.tile(tf.expand_dims(w, 1), (1, num_stages, 1))\n    return keras.Model(z, w, name=\"mapping\")\n\n\nclass Generator:\n    def __init__(self, start_res_log2, target_res_log2):\n        self.start_res_log2 = start_res_log2\n        self.target_res_log2 = target_res_log2\n        self.num_stages = target_res_log2 - start_res_log2 + 1\n        # list of generator blocks at increasing resolution\n        self.g_blocks = []\n        # list of layers to convert g_block activation to RGB\n        self.to_rgb = []\n        # list of noise input of different resolutions into g_blocks\n        self.noise_inputs = []\n        # filter size to use at each stage, keys are log2(resolution)\n        self.filter_nums = {\n            0: 512,\n            1: 512,\n            2: 512,  # 4x4\n            3: 512,  # 8x8\n            4: 512,  # 16x16\n            5: 512,  # 32x32\n            6: 256,  # 64x64\n            7: 128,  # 128x128\n            8: 64,  # 256x256\n            9: 32,  # 512x512\n            10: 16,\n        }  # 1024x1024\n\n        start_res = 2 ** start_res_log2\n        self.input_shape = (start_res, start_res, self.filter_nums[start_res_log2])\n        self.g_input = layers.Input(self.input_shape, name=\"generator_input\")\n\n        for i in range(start_res_log2, target_res_log2 + 1):\n            filter_num = self.filter_nums[i]\n            res = 2 ** i\n            self.noise_inputs.append(\n                layers.Input(shape=(res, res, 1), name=f\"noise_{res}x{res}\")\n            )\n            to_rgb = Sequential(\n                [\n                    layers.InputLayer(input_shape=(res, res, filter_num)),\n                    EqualizedConv(3, 1, gain=1),\n                ],\n                name=f\"to_rgb_{res}x{res}\",\n            )\n            self.to_rgb.append(to_rgb)\n            is_base = i == self.start_res_log2\n            if is_base:\n                input_shape = (res, res, self.filter_nums[i - 1])\n            else:\n                input_shape = (2 ** (i - 1), 2 ** (i - 1), self.filter_nums[i - 1])\n            g_block = self.build_block(\n                filter_num, res=res, input_shape=input_shape, is_base=is_base\n            )\n            self.g_blocks.append(g_block)\n\n    def build_block(self, filter_num, res, input_shape, is_base):\n        input_tensor = layers.Input(shape=input_shape, name=f\"g_{res}\")\n        noise = layers.Input(shape=(res, res, 1), name=f\"noise_{res}\")\n        w = layers.Input(shape=512)\n        x = input_tensor\n\n        if not is_base:\n            x = layers.UpSampling2D((2, 2))(x)\n            x = EqualizedConv(filter_num, 3)(x)\n\n        x = AddNoise()([x, noise])\n        x = layers.LeakyReLU(0.2)(x)\n        x = InstanceNormalization()(x)\n        x = AdaIN()([x, w])\n\n        x = EqualizedConv(filter_num, 3)(x)\n        x = AddNoise()([x, noise])\n        x = layers.LeakyReLU(0.2)(x)\n        x = InstanceNormalization()(x)\n        x = AdaIN()([x, w])\n        return keras.Model([input_tensor, w, noise], x, name=f\"genblock_{res}x{res}\")\n\n    def grow(self, res_log2):\n        res = 2 ** res_log2\n\n        num_stages = res_log2 - self.start_res_log2 + 1\n        w = layers.Input(shape=(self.num_stages, 512), name=\"w\")\n\n        alpha = layers.Input(shape=(1), name=\"g_alpha\")\n        x = self.g_blocks[0]([self.g_input, w[:, 0], self.noise_inputs[0]])\n\n        if num_stages == 1:\n            rgb = self.to_rgb[0](x)\n        else:\n            for i in range(1, num_stages - 1):\n\n                x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n\n            old_rgb = self.to_rgb[num_stages - 2](x)\n            old_rgb = layers.UpSampling2D((2, 2))(old_rgb)\n\n            i = num_stages - 1\n            x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n\n            new_rgb = self.to_rgb[i](x)\n\n            rgb = fade_in(alpha[0], new_rgb, old_rgb)\n\n        return keras.Model(\n            [self.g_input, w, self.noise_inputs, alpha],\n            rgb,\n            name=f\"generator_{res}_x_{res}\",\n        )\n\n\nclass Discriminator:\n    def __init__(self, start_res_log2, target_res_log2):\n        self.start_res_log2 = start_res_log2\n        self.target_res_log2 = target_res_log2\n        self.num_stages = target_res_log2 - start_res_log2 + 1\n        # filter size to use at each stage, keys are log2(resolution)\n        self.filter_nums = {\n            0: 512,\n            1: 512,\n            2: 512,  # 4x4\n            3: 512,  # 8x8\n            4: 512,  # 16x16\n            5: 512,  # 32x32\n            6: 256,  # 64x64\n            7: 128,  # 128x128\n            8: 64,  # 256x256\n            9: 32,  # 512x512\n            10: 16,\n        }  # 1024x1024\n        # list of discriminator blocks at increasing resolution\n        self.d_blocks = []\n        # list of layers to convert RGB into activation for d_blocks inputs\n        self.from_rgb = []\n\n        for res_log2 in range(self.start_res_log2, self.target_res_log2 + 1):\n            res = 2 ** res_log2\n            filter_num = self.filter_nums[res_log2]\n            from_rgb = Sequential(\n                [\n                    layers.InputLayer(\n                        input_shape=(res, res, 3), name=f\"from_rgb_input_{res}\"\n                    ),\n                    EqualizedConv(filter_num, 1),\n                    layers.LeakyReLU(0.2),\n                ],\n                name=f\"from_rgb_{res}\",\n            )\n\n            self.from_rgb.append(from_rgb)\n\n            input_shape = (res, res, filter_num)\n            if len(self.d_blocks) == 0:\n                d_block = self.build_base(filter_num, res)\n            else:\n                d_block = self.build_block(\n                    filter_num, self.filter_nums[res_log2 - 1], res\n                )\n\n            self.d_blocks.append(d_block)\n\n    def build_base(self, filter_num, res):\n        input_tensor = layers.Input(shape=(res, res, filter_num), name=f\"d_{res}\")\n        x = minibatch_std(input_tensor)\n        x = EqualizedConv(filter_num, 3)(x)\n        x = layers.LeakyReLU(0.2)(x)\n        x = layers.Flatten()(x)\n        x = EqualizedDense(filter_num)(x)\n        x = layers.LeakyReLU(0.2)(x)\n        x = EqualizedDense(1)(x)\n        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n\n    def build_block(self, filter_num_1, filter_num_2, res):\n        input_tensor = layers.Input(shape=(res, res, filter_num_1), name=f\"d_{res}\")\n        x = EqualizedConv(filter_num_1, 3)(input_tensor)\n        x = layers.LeakyReLU(0.2)(x)\n        x = EqualizedConv(filter_num_2)(x)\n        x = layers.LeakyReLU(0.2)(x)\n        x = layers.AveragePooling2D((2, 2))(x)\n        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n\n    def grow(self, res_log2):\n        res = 2 ** res_log2\n        idx = res_log2 - self.start_res_log2\n        alpha = layers.Input(shape=(1), name=\"d_alpha\")\n        input_image = layers.Input(shape=(res, res, 3), name=\"input_image\")\n        x = self.from_rgb[idx](input_image)\n        x = self.d_blocks[idx](x)\n        if idx > 0:\n            idx -= 1\n            downsized_image = layers.AveragePooling2D((2, 2))(input_image)\n            y = self.from_rgb[idx](downsized_image)\n            x = fade_in(alpha[0], x, y)\n\n            for i in range(idx, -1, -1):\n                x = self.d_blocks[i](x)\n        return keras.Model([input_image, alpha], x, name=f\"discriminator_{res}_x_{res}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:23:00.971043Z","iopub.execute_input":"2024-07-11T07:23:00.971452Z","iopub.status.idle":"2024-07-11T07:23:01.005114Z","shell.execute_reply.started":"2024-07-11T07:23:00.971422Z","shell.execute_reply":"2024-07-11T07:23:01.003828Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\nclass StyleGAN(tf.keras.Model):\n    def __init__(self, z_dim=512, target_res=64, start_res=4):\n        super().__init__()\n        self.z_dim = z_dim\n\n        self.target_res_log2 = log2(target_res)\n        self.start_res_log2 = log2(start_res)\n        self.current_res_log2 = self.target_res_log2\n        self.num_stages = self.target_res_log2 - self.start_res_log2 + 1\n\n        self.alpha = tf.Variable(1.0, dtype=tf.float32, trainable=False, name=\"alpha\")\n\n        self.mapping = Mapping(num_stages=self.num_stages)\n        self.d_builder = Discriminator(self.start_res_log2, self.target_res_log2)\n        self.g_builder = Generator(self.start_res_log2, self.target_res_log2)\n        self.g_input_shape = self.g_builder.input_shape\n\n        self.phase = None\n        self.train_step_counter = tf.Variable(0, dtype=tf.int32, trainable=False)\n\n        self.loss_weights = {\"gradient_penalty\": 10, \"drift\": 0.001}\n\n    def grow_model(self, res):\n        tf.keras.backend.clear_session()\n        res_log2 = log2(res)\n        self.generator = self.g_builder.grow(res_log2)\n        self.discriminator = self.d_builder.grow(res_log2)\n        self.current_res_log2 = res_log2\n        print(f\"\\nModel resolution:{res}x{res}\")\n\n    def compile(\n        self, steps_per_epoch, phase, res, d_optimizer, g_optimizer, *args, **kwargs\n    ):\n        self.loss_weights = kwargs.pop(\"loss_weights\", self.loss_weights)\n        self.steps_per_epoch = steps_per_epoch\n        if res != 2 ** self.current_res_log2:\n            self.grow_model(res)\n            self.d_optimizer = d_optimizer\n            self.g_optimizer = g_optimizer\n\n        self.train_step_counter.assign(0)\n        self.phase = phase\n        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n        super().compile(*args, **kwargs)\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def generate_noise(self, batch_size):\n        noise = [\n            tf.random.normal((batch_size, 2 ** res, 2 ** res, 1))\n            for res in range(self.start_res_log2, self.target_res_log2 + 1)\n        ]\n        return noise\n\n    def gradient_loss(self, grad):\n        loss = tf.square(grad)\n        loss = tf.reduce_sum(loss, axis=tf.range(1, tf.size(tf.shape(loss))))\n        loss = tf.sqrt(loss)\n        loss = tf.reduce_mean(tf.square(loss - 1))\n        return loss\n\n    def train_step(self, real_images):\n\n        self.train_step_counter.assign_add(1)\n\n        if self.phase == \"TRANSITION\":\n            self.alpha.assign(\n                tf.cast(self.train_step_counter / self.steps_per_epoch, tf.float32)\n            )\n        elif self.phase == \"STABLE\":\n            self.alpha.assign(1.0)\n        else:\n            raise NotImplementedError\n        alpha = tf.expand_dims(self.alpha, 0)\n        batch_size = tf.shape(real_images)[0]\n        real_labels = tf.ones(batch_size)\n        fake_labels = -tf.ones(batch_size)\n\n        z = tf.random.normal((batch_size, self.z_dim))\n        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n        noise = self.generate_noise(batch_size)\n\n        # generator\n        with tf.GradientTape() as g_tape:\n            w = self.mapping(z)\n            fake_images = self.generator([const_input, w, noise, alpha])\n            pred_fake = self.discriminator([fake_images, alpha])\n            g_loss = wasserstein_loss(real_labels, pred_fake)\n\n            trainable_weights = (\n                self.mapping.trainable_weights + self.generator.trainable_weights\n            )\n            gradients = g_tape.gradient(g_loss, trainable_weights)\n            self.g_optimizer.apply_gradients(zip(gradients, trainable_weights))\n\n        # discriminator\n        with tf.GradientTape() as gradient_tape, tf.GradientTape() as total_tape:\n            # forward pass\n            pred_fake = self.discriminator([fake_images, alpha])\n            pred_real = self.discriminator([real_images, alpha])\n\n            epsilon = tf.random.uniform((batch_size, 1, 1, 1))\n            interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n            gradient_tape.watch(interpolates)\n            pred_fake_grad = self.discriminator([interpolates, alpha])\n\n            # calculate losses\n            loss_fake = wasserstein_loss(fake_labels, pred_fake)\n            loss_real = wasserstein_loss(real_labels, pred_real)\n            loss_fake_grad = wasserstein_loss(fake_labels, pred_fake_grad)\n\n            # gradient penalty\n            gradients_fake = gradient_tape.gradient(loss_fake_grad, [interpolates])\n            gradient_penalty = self.loss_weights[\n                \"gradient_penalty\"\n            ] * self.gradient_loss(gradients_fake)\n\n            # drift loss\n            all_pred = tf.concat([pred_fake, pred_real], axis=0)\n            drift_loss = self.loss_weights[\"drift\"] * tf.reduce_mean(all_pred ** 2)\n\n            d_loss = loss_fake + loss_real + gradient_penalty + drift_loss\n\n            gradients = total_tape.gradient(\n                d_loss, self.discriminator.trainable_weights\n            )\n            self.d_optimizer.apply_gradients(\n                zip(gradients, self.discriminator.trainable_weights)\n            )\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\n            \"d_loss\": self.d_loss_metric.result(),\n            \"g_loss\": self.g_loss_metric.result(),\n        }\n\n    def call(self, inputs: dict()):\n        style_code = inputs.get(\"style_code\", None)\n        z = inputs.get(\"z\", None)\n        noise = inputs.get(\"noise\", None)\n        batch_size = inputs.get(\"batch_size\", 1)\n        alpha = inputs.get(\"alpha\", 1.0)\n        alpha = tf.expand_dims(alpha, 0)\n        if style_code is None:\n            if z is None:\n                z = tf.random.normal((batch_size, self.z_dim))\n            style_code = self.mapping(z)\n\n        if noise is None:\n            noise = self.generate_noise(batch_size)\n\n        # self.alpha.assign(alpha)\n\n        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n        images = self.generator([const_input, style_code, noise, alpha])\n        images = np.clip((images * 0.5 + 0.5) * 255, 0, 255).astype(np.uint8)\n\n        return images\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:30:16.092056Z","iopub.execute_input":"2024-07-11T07:30:16.092474Z","iopub.status.idle":"2024-07-11T07:30:16.120085Z","shell.execute_reply.started":"2024-07-11T07:30:16.092433Z","shell.execute_reply":"2024-07-11T07:30:16.118892Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"START_RES = 4\nTARGET_RES = 128\n\nstyle_gan = StyleGAN(start_res=START_RES, target_res=TARGET_RES)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T07:30:28.461637Z","iopub.execute_input":"2024-07-11T07:30:28.462206Z","iopub.status.idle":"2024-07-11T07:30:28.590333Z","shell.execute_reply.started":"2024-07-11T07:30:28.462171Z","shell.execute_reply":"2024-07-11T07:30:28.588663Z"},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m START_RES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      2\u001b[0m TARGET_RES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m----> 4\u001b[0m style_gan \u001b[38;5;241m=\u001b[39m \u001b[43mStyleGAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTART_RES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_res\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_RES\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[25], line 13\u001b[0m, in \u001b[0;36mStyleGAN.__init__\u001b[0;34m(self, z_dim, target_res, start_res)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_res_log2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_res_log2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m1.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping \u001b[38;5;241m=\u001b[39m \u001b[43mMapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_stages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_builder \u001b[38;5;241m=\u001b[39m Discriminator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_res_log2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_res_log2)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_builder \u001b[38;5;241m=\u001b[39m Generator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_res_log2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_res_log2)\n","Cell \u001b[0;32mIn[16], line 2\u001b[0m, in \u001b[0;36mMapping\u001b[0;34m(num_stages, input_shape)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMapping\u001b[39m(num_stages, input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     w \u001b[38;5;241m=\u001b[39m pixel_norm(z)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:143\u001b[0m, in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, dtype, sparse, batch_shape, name, tensor)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.Input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.Input\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mInput\u001b[39m(\n\u001b[1;32m     93\u001b[0m     shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    101\u001b[0m ):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used to instantiate a Keras tensor.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    A Keras tensor is a symbolic tensor-like object, which we augment with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    certain attributes that allow us to build a Keras model just by knowing the\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    inputs and outputs of the model.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    For instance, if `a`, `b` and `c` are Keras tensors,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    it becomes possible to do:\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    `model = Model(input=[a, b], output=c)`\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        shape: A shape tuple (tuple of integers or `None` objects),\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m            not including the batch size.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m            For instance, `shape=(32,)` indicates that the expected input\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m            will be batches of 32-dimensional vectors. Elements of this tuple\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m            can be `None`; `None` elements represent dimensions where the shape\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m            is not known and may vary (e.g. sequence length).\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m        batch_size: Optional static batch size (integer).\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        dtype: The data type expected by the input, as a string\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m            (e.g. `\"float32\"`, `\"int32\"`...)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m        sparse: A boolean specifying whether the expected input will be sparse\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m            tensors. Note that, if `sparse` is `False`, sparse tensors can still\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m            be passed into the input - they will be densified with a default\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m            value of 0. This feature is only supported with the TensorFlow\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m            backend. Defaults to `False`.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m        name: Optional name string for the layer.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m            Should be unique in a model (do not reuse the same name twice).\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m            It will be autogenerated if it isn't provided.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        tensor: Optional existing tensor to wrap into the `Input` layer.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m            If set, the layer will use this tensor rather\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m            than creating a new placeholder tensor.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m        optional: Boolean, whether the input is optional or not.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m            An optional input can accept `None` values.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m      A Keras tensor.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    # This is a logistic regression in Keras\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;124;03m    x = Input(shape=(32,))\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    y = Dense(16, activation='softmax')(x)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    model = Model(x, y)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     layer \u001b[38;5;241m=\u001b[39m InputLayer(\n\u001b[1;32m    149\u001b[0m         shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[1;32m    150\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m         optional\u001b[38;5;241m=\u001b[39moptional,\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer\u001b[38;5;241m.\u001b[39moutput\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:46\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[0;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, name, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass a `shape` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(shape)\n\u001b[1;32m     48\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m shape\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/common/variables.py:530\u001b[0m, in \u001b[0;36mstandardize_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined shapes are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(shape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, tf\u001b[38;5;241m.\u001b[39mTensorShape):\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;66;03m# `tf.TensorShape` may contain `Dimension` objects.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;66;03m# We need to convert the items in it to either int or `None`\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: Cannot convert '512' to a shape."],"ename":"ValueError","evalue":"Cannot convert '512' to a shape.","output_type":"error"}]},{"cell_type":"code","source":"\ndef train(\n    start_res=START_RES,\n    target_res=TARGET_RES,\n    steps_per_epoch=5000,\n    display_images=True,\n):\n    opt_cfg = {\"learning_rate\": 1e-3, \"beta_1\": 0.0, \"beta_2\": 0.99, \"epsilon\": 1e-8}\n\n    val_batch_size = 16\n    val_z = tf.random.normal((val_batch_size, style_gan.z_dim))\n    val_noise = style_gan.generate_noise(val_batch_size)\n\n    start_res_log2 = int(np.log2(start_res))\n    target_res_log2 = int(np.log2(target_res))\n\n    for res_log2 in range(start_res_log2, target_res_log2 + 1):\n        res = 2 ** res_log2\n        for phase in [\"TRANSITION\", \"STABLE\"]:\n            if res == start_res and phase == \"TRANSITION\":\n                continue\n\n            train_dl = create_dataloader(res)\n\n            steps = int(train_step_ratio[res_log2] * steps_per_epoch)\n\n            style_gan.compile(\n                d_optimizer=tf.keras.optimizers.legacy.Adam(**opt_cfg),\n                g_optimizer=tf.keras.optimizers.legacy.Adam(**opt_cfg),\n                loss_weights={\"gradient_penalty\": 10, \"drift\": 0.001},\n                steps_per_epoch=steps,\n                res=res,\n                phase=phase,\n                run_eagerly=False,\n            )\n\n            prefix = f\"res_{res}x{res}_{style_gan.phase}\"\n\n            ckpt_cb = keras.callbacks.ModelCheckpoint(\n                f\"checkpoints/stylegan_{res}x{res}.ckpt\",\n                save_weights_only=True,\n                verbose=0,\n            )\n            print(phase)\n            style_gan.fit(\n                train_dl, epochs=1, steps_per_epoch=steps, callbacks=[ckpt_cb]\n            )\n\n            if display_images:\n                images = style_gan({\"z\": val_z, \"noise\": val_noise, \"alpha\": 1.0})\n                plot_images(images, res_log2)\n","metadata":{},"execution_count":null,"outputs":[]}]}